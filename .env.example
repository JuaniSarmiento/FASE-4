# ============================================================================
# Phoenix MVP - Environment Variables Template  
# ============================================================================
# Copy this file to .env and fill in your values
# NEVER commit .env to git (it's in .gitignore)

# ============================================================================
# DATABASE
# ============================================================================
DATABASE_URL=postgresql://ai_native:ai_native_password@localhost:5432/ai_native
DB_POOL_SIZE=80
DB_MAX_OVERFLOW=80
DB_POOL_TIMEOUT=5
DB_POOL_RECYCLE=3600

# ============================================================================
# REDIS CACHE
# ============================================================================
REDIS_URL=redis://localhost:6379/0
LLM_CACHE_ENABLED=true
LLM_CACHE_TTL=3600
LLM_CACHE_MAX_ENTRIES=1000

# ============================================================================
# LLM PROVIDER - OLLAMA (Local, Free, Privacy-First)
# ============================================================================
LLM_PROVIDER=ollama

# Ollama server URL (default: localhost)
# For Docker: http://ollama:11434
# For local: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Model to use (install with: ollama pull <model>)
# Options: phi3, llama2, mistral, codellama, gemma:7b, etc.
# Recommended: phi3 (Microsoft Phi-3: 3.8B params, fast, efficient)
OLLAMA_MODEL=phi3

# Generation temperature (0.0 = deterministic, 1.0 = creative)
OLLAMA_TEMPERATURE=0.7

# Request timeout in seconds (optional)
# OLLAMA_TIMEOUT=120

# ============================================================================
# SECURITY
# ============================================================================
# JWT Secret Key (REQUIRED in production)
# Generate with: python -c 'import secrets; print(secrets.token_urlsafe(32))'
JWT_SECRET_KEY=CHANGE_THIS_IN_PRODUCTION

JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# ============================================================================
# APPLICATION
# ============================================================================
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO

# ============================================================================
# CORS (Frontend origins)
# ============================================================================
# Include all frontend development ports (3000, 3001, 5173, 8080)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001,http://localhost:5173,http://localhost:8080

# ============================================================================
# MONITORING (Optional - for production)
# ============================================================================
# PROMETHEUS_PORT=9090
# GRAFANA_PORT=3001

# ============================================================================
# QUICK START
# ============================================================================
# 1. Install Ollama: https://ollama.ai
# 2. Download a model: ollama pull llama2
# 3. Start Ollama: ollama serve
# 4. Copy this file: cp .env.example .env
# 5. Run Phoenix: python scripts/run_api.py
# 6. Access API docs: http://localhost:8000/docs
#
# For Docker deployment:
#   docker-compose --profile ollama up -d
#
# Documentation:
#   - Quick Start: OLLAMA_QUICKSTART.md
#   - Full Guide: OLLAMA_INTEGRATION_GUIDE.md
